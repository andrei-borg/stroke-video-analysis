{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from keras.applications.resnet import ResNet152, ResNet50, ResNet101\n",
    "from keras.models import Model\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import decomposition, svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import (\n",
    "    Conv2D,\n",
    "    MaxPool2D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    InputLayer,\n",
    "    Activation,\n",
    "    BatchNormalization,\n",
    "    GlobalAveragePooling2D,\n",
    "    Dropout,\n",
    ")\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(\n",
    "    cm, classes, normalize=False, title=\"Confusion matrix\", cmap=plt.cm.Blues\n",
    "):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print(\"Confusion matrix, without normalization\")\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(\n",
    "            j,\n",
    "            i,\n",
    "            cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.show()\n",
    "\n",
    "input_shape = (465, 930, 3)\n",
    "\n",
    "# Define hyperparameters and their values for GridSearch\n",
    "param_grid = {\n",
    "    'learning_rate': [0.0001, 0.001],\n",
    "    'hidden_layers': [1, 2],\n",
    "}\n",
    "\n",
    "combinations = [[0,0], [0,1], [1,0], [1,1]]\n",
    "\n",
    "# Define hyperparameters and their values for GridSearch\n",
    "param_grid2 = {\n",
    "    'learning_rate': [0.0001, 0.001],\n",
    "    'dropout_rate':  [0, 0.2],\n",
    "    'hidden_layers': [1, 2],\n",
    "}\n",
    "\n",
    "combinations2 = [[0,0,0], [0,0,1], [0,1,0], [0,1,1], [1,0,0], [1,0,1], [1,1,0], [1,1,1]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3537 images belonging to 2 classes.\n",
      "Found 585 images belonging to 2 classes.\n",
      "Found 51 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_path = \"rp_data/train\"\n",
    "valid_path = \"rp_data/valid\"\n",
    "test_path = \"rp_data/test\"\n",
    "\n",
    "# Define image size and number of channels\n",
    "img_height = 465\n",
    "img_width = 930\n",
    "\n",
    "# Change preprocess_input to the corresponding classifier (vgg16, resnet etc.)\n",
    "train_batches = ImageDataGenerator(\n",
    "    tf.keras.applications.vgg16.preprocess_input\n",
    ").flow_from_directory(\n",
    "    directory=train_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    classes=[\"normal\", \"stroke\"],\n",
    "    batch_size=16,\n",
    ")\n",
    "\n",
    "valid_batches = ImageDataGenerator(\n",
    "    tf.keras.applications.vgg16.preprocess_input\n",
    ").flow_from_directory(\n",
    "    directory=valid_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    classes=[\"normal\", \"stroke\"],\n",
    "    batch_size=16,\n",
    ")\n",
    "test_batches = ImageDataGenerator(\n",
    "    tf.keras.applications.vgg16.preprocess_input\n",
    ").flow_from_directory(\n",
    "    directory=test_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    classes=[\"normal\", \"stroke\"],\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique classes: 2\n",
      "Training images and labels: (465, 930) (3537,)\n"
     ]
    }
   ],
   "source": [
    "# Using numpy arrays instead:\n",
    "\n",
    "train_path_normal = \"rp_data/train/normal\"\n",
    "train_path_stroke = \"rp_data/train/stroke\"\n",
    "valid_path_normal = \"rp_data/valid/normal\"\n",
    "valid_path_stroke = \"rp_data/valid/stroke\"\n",
    "test_path_normal = \"rp_data/test/normal\"\n",
    "test_path_stroke = \"rp_data/test/stroke\"\n",
    "\n",
    "# Define image size and number of channels\n",
    "height = 465\n",
    "width = 930\n",
    "\n",
    "# --- Training dataset --- #\n",
    "\n",
    "train_images = []\n",
    "train_labels = []\n",
    "\n",
    "for filename in os.listdir(train_path_normal):\n",
    "    # Load the image using PIL\n",
    "    img = Image.open(os.path.join(train_path_normal, filename)).convert('L')\n",
    "    # Convert the image to a numpy array and append to train_images array\n",
    "    train_images.append(np.array(img).astype(np.float32)/255.)\n",
    "    # Append the label to the train_labels list\n",
    "    train_labels.append(0)\n",
    "\n",
    "for filename in os.listdir(train_path_stroke):\n",
    "    # Load the image using PIL\n",
    "    img = Image.open(os.path.join(train_path_stroke, filename)).convert('L')\n",
    "    # Convert the image to a numpy array and append to train_images array\n",
    "    train_images.append(np.array(img).astype(np.float32)/255.)\n",
    "    # Append the label to the train_labels list\n",
    "    train_labels.append(1)\n",
    "\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "train_images = train_images[0].reshape(height, width)\n",
    "\n",
    "num_classes = len(np.unique(train_labels))\n",
    "print(\"Number of unique classes:\", num_classes)\n",
    "print(\"Training images and labels:\", train_images.shape, train_labels.shape)\n",
    "\n",
    "# Shuffle:\n",
    "\n",
    "# Combine the train_images and train_labels list using zip\n",
    "combined = list(zip(train_images, train_labels))\n",
    "# Shuffle the combined list\n",
    "random.shuffle(combined)\n",
    "# Separate the shuffled images and labels list\n",
    "train_images, train_labels = zip(*combined)\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "#print(train_labels, train_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique classes: 2\n",
      "Validation images and labels: (465, 930) (585,)\n"
     ]
    }
   ],
   "source": [
    "# --- Validation dataset --- #\n",
    "\n",
    "valid_images = []\n",
    "valid_labels = []\n",
    "\n",
    "for filename in os.listdir(valid_path_normal):\n",
    "    # Load the image using PIL\n",
    "    img = Image.open(os.path.join(valid_path_normal, filename)).convert('L')\n",
    "    # Convert the image to a numpy array and append to train_images array\n",
    "    valid_images.append(np.array(img).astype(np.float32)/255.)\n",
    "    # Append the label to the train_labels list\n",
    "    valid_labels.append(0)\n",
    "\n",
    "for filename in os.listdir(valid_path_stroke):\n",
    "    # Load the image using PIL\n",
    "    img = Image.open(os.path.join(valid_path_stroke, filename)).convert('L')\n",
    "    # Convert the image to a numpy array and append to train_images array\n",
    "    valid_images.append(np.array(img).astype(np.float32)/255.)\n",
    "    # Append the label to the train_labels list\n",
    "    valid_labels.append(1)\n",
    "\n",
    "valid_images = np.array(valid_images)\n",
    "valid_labels = np.array(valid_labels)\n",
    "valid_images = valid_images[0].reshape(height, width)\n",
    "\n",
    "num_classes = len(np.unique(valid_labels))\n",
    "print(\"Number of unique classes:\", num_classes)\n",
    "\n",
    "print(\"Validation images and labels:\", valid_images.shape, valid_labels.shape)\n",
    "\n",
    "# Shuffle:\n",
    "\n",
    "# Combine the train_images and train_labels list using zip\n",
    "combined = list(zip(valid_images, valid_labels))\n",
    "# Shuffle the combined list\n",
    "random.shuffle(combined)\n",
    "# Separate the shuffled images and labels list\n",
    "valid_images, valid_labels = zip(*combined)\n",
    "valid_images = np.array(valid_images)\n",
    "valid_labels = np.array(valid_labels)\n",
    "\n",
    "#print(valid_labels, valid_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing images and labels: (465, 930) (51,)\n"
     ]
    }
   ],
   "source": [
    "# --- Test dataset --- #\n",
    "\n",
    "test_images = []\n",
    "test_labels = []\n",
    "\n",
    "for filename in os.listdir(test_path_normal):\n",
    "    # Load the image using PIL\n",
    "    img = Image.open(os.path.join(test_path_normal, filename)).convert('L')\n",
    "    # Convert the image to a numpy array and append to train_images array\n",
    "    test_images.append(np.array(img).astype(np.float32)/255.)\n",
    "    # Append the label to the train_labels list\n",
    "    test_labels.append(0)\n",
    "\n",
    "for filename in os.listdir(test_path_stroke):\n",
    "    # Load the image using PIL\n",
    "    img = Image.open(os.path.join(test_path_stroke, filename)).convert('L')\n",
    "    # Convert the image to a numpy array and append to train_images array\n",
    "    test_images.append(np.array(img).astype(np.float32)/255.)\n",
    "    # Append the label to the train_labels list\n",
    "    test_labels.append(1)\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)\n",
    "test_images = test_images[0].reshape(height, width)\n",
    "\n",
    "print(\"Testing images and labels:\", test_images.shape, test_labels.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n",
      "Layers: 1\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 465, 930, 32)      2432      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 232, 465, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 232, 465, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 116, 232, 64)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " global_average_pooling2d_22  (None, 64)               0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,218\n",
      "Trainable params: 25,218\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AndreiBorg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\preprocessing\\image.py:1863: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "222/222 - 385s - loss: 0.4985 - accuracy: 0.8015 - val_loss: 0.4187 - val_accuracy: 0.9197 - 385s/epoch - 2s/step\n",
      "Epoch 2/10\n",
      "222/222 - 390s - loss: 0.4078 - accuracy: 0.8230 - val_loss: 0.4299 - val_accuracy: 0.9179 - 390s/epoch - 2s/step\n",
      "Epoch 3/10\n",
      "222/222 - 387s - loss: 0.3899 - accuracy: 0.8312 - val_loss: 0.4040 - val_accuracy: 0.8650 - 387s/epoch - 2s/step\n",
      "Epoch 4/10\n",
      "222/222 - 370s - loss: 0.3657 - accuracy: 0.8439 - val_loss: 0.3793 - val_accuracy: 0.9179 - 370s/epoch - 2s/step\n",
      "Epoch 5/10\n",
      "222/222 - 365s - loss: 0.3635 - accuracy: 0.8504 - val_loss: 0.3999 - val_accuracy: 0.9145 - 365s/epoch - 2s/step\n",
      "Epoch 6/10\n",
      "222/222 - 365s - loss: 0.3521 - accuracy: 0.8490 - val_loss: 0.4944 - val_accuracy: 0.7214 - 365s/epoch - 2s/step\n",
      "Epoch 7/10\n",
      "222/222 - 365s - loss: 0.3409 - accuracy: 0.8519 - val_loss: 0.3633 - val_accuracy: 0.9128 - 365s/epoch - 2s/step\n",
      "Epoch 8/10\n",
      "222/222 - 364s - loss: 0.3209 - accuracy: 0.8671 - val_loss: 0.3531 - val_accuracy: 0.9368 - 364s/epoch - 2s/step\n",
      "Epoch 9/10\n",
      "222/222 - 365s - loss: 0.3187 - accuracy: 0.8671 - val_loss: 0.3362 - val_accuracy: 0.9333 - 365s/epoch - 2s/step\n",
      "Epoch 10/10\n",
      "222/222 - 365s - loss: 0.3113 - accuracy: 0.8731 - val_loss: 0.4026 - val_accuracy: 0.8359 - 365s/epoch - 2s/step\n",
      "Test loss: 1.2563328742980957, Test accuracy 0.6470588445663452\n",
      "Learning rate: 0.0001\n",
      "Layers: 2\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 465, 930, 32)      2432      \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 232, 465, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 232, 465, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 116, 232, 64)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 116, 232, 64)      36928     \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 58, 116, 64)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " global_average_pooling2d_23  (None, 64)               0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 62,146\n",
      "Trainable params: 62,146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "222/222 - 373s - loss: 0.4132 - accuracy: 0.8126 - val_loss: 0.6100 - val_accuracy: 0.7162 - 373s/epoch - 2s/step\n",
      "Epoch 2/10\n",
      "222/222 - 370s - loss: 0.3678 - accuracy: 0.8428 - val_loss: 0.3551 - val_accuracy: 0.9419 - 370s/epoch - 2s/step\n",
      "Epoch 3/10\n",
      "222/222 - 370s - loss: 0.3451 - accuracy: 0.8572 - val_loss: 0.3298 - val_accuracy: 0.9436 - 370s/epoch - 2s/step\n",
      "Epoch 4/10\n",
      "222/222 - 370s - loss: 0.3222 - accuracy: 0.8756 - val_loss: 0.3323 - val_accuracy: 0.9436 - 370s/epoch - 2s/step\n",
      "Epoch 5/10\n",
      "222/222 - 370s - loss: 0.3169 - accuracy: 0.8764 - val_loss: 0.3329 - val_accuracy: 0.9282 - 370s/epoch - 2s/step\n",
      "Epoch 6/10\n",
      "222/222 - 370s - loss: 0.2976 - accuracy: 0.8858 - val_loss: 0.3510 - val_accuracy: 0.9162 - 370s/epoch - 2s/step\n",
      "Epoch 7/10\n",
      "222/222 - 370s - loss: 0.3261 - accuracy: 0.8685 - val_loss: 0.3114 - val_accuracy: 0.9436 - 370s/epoch - 2s/step\n",
      "Epoch 8/10\n",
      "222/222 - 370s - loss: 0.2892 - accuracy: 0.8880 - val_loss: 0.3685 - val_accuracy: 0.8821 - 370s/epoch - 2s/step\n",
      "Epoch 9/10\n",
      "222/222 - 370s - loss: 0.2843 - accuracy: 0.8900 - val_loss: 0.3230 - val_accuracy: 0.9282 - 370s/epoch - 2s/step\n",
      "Epoch 10/10\n",
      "222/222 - 370s - loss: 0.2860 - accuracy: 0.8818 - val_loss: 0.3990 - val_accuracy: 0.8906 - 370s/epoch - 2s/step\n",
      "Test loss: 0.9115867614746094, Test accuracy 0.8039215803146362\n",
      "Learning rate: 0.001\n",
      "Layers: 1\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_7 (Conv2D)           (None, 465, 930, 32)      2432      \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 232, 465, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 232, 465, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 116, 232, 64)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " global_average_pooling2d_24  (None, 64)               0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_70 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_71 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,218\n",
      "Trainable params: 25,218\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "222/222 - 367s - loss: 0.5593 - accuracy: 0.8010 - val_loss: 0.4522 - val_accuracy: 0.8051 - 367s/epoch - 2s/step\n",
      "Epoch 2/10\n",
      "222/222 - 366s - loss: 0.3913 - accuracy: 0.8244 - val_loss: 0.3770 - val_accuracy: 0.9385 - 366s/epoch - 2s/step\n",
      "Epoch 3/10\n",
      "222/222 - 365s - loss: 0.4043 - accuracy: 0.8165 - val_loss: 0.4019 - val_accuracy: 0.8547 - 365s/epoch - 2s/step\n",
      "Epoch 4/10\n",
      "222/222 - 369s - loss: 0.3307 - accuracy: 0.8671 - val_loss: 0.3548 - val_accuracy: 0.9368 - 369s/epoch - 2s/step\n",
      "Epoch 5/10\n",
      "222/222 - 360s - loss: 0.3234 - accuracy: 0.8643 - val_loss: 0.3293 - val_accuracy: 0.8974 - 360s/epoch - 2s/step\n",
      "Epoch 6/10\n",
      "222/222 - 381s - loss: 0.3143 - accuracy: 0.8663 - val_loss: 0.3366 - val_accuracy: 0.9043 - 381s/epoch - 2s/step\n",
      "Epoch 7/10\n",
      "222/222 - 370s - loss: 0.2822 - accuracy: 0.8824 - val_loss: 0.3654 - val_accuracy: 0.8974 - 370s/epoch - 2s/step\n",
      "Epoch 8/10\n",
      "222/222 - 378s - loss: 0.2947 - accuracy: 0.8796 - val_loss: 0.3225 - val_accuracy: 0.9265 - 378s/epoch - 2s/step\n",
      "Epoch 9/10\n",
      "222/222 - 368s - loss: 0.2789 - accuracy: 0.8869 - val_loss: 0.4807 - val_accuracy: 0.8034 - 368s/epoch - 2s/step\n",
      "Epoch 10/10\n",
      "222/222 - 385s - loss: 0.2624 - accuracy: 0.8900 - val_loss: 0.4452 - val_accuracy: 0.7949 - 385s/epoch - 2s/step\n",
      "Test loss: 2.1073577404022217, Test accuracy 0.6274510025978088\n",
      "Learning rate: 0.001\n",
      "Layers: 2\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_9 (Conv2D)           (None, 465, 930, 32)      2432      \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 232, 465, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 232, 465, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 116, 232, 64)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 116, 232, 64)      36928     \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 58, 116, 64)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d_25  (None, 64)               0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 62,146\n",
      "Trainable params: 62,146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "222/222 - 379s - loss: 0.6013 - accuracy: 0.7727 - val_loss: 0.4496 - val_accuracy: 0.8051 - 379s/epoch - 2s/step\n",
      "Epoch 2/10\n",
      "222/222 - 363s - loss: 0.3730 - accuracy: 0.8369 - val_loss: 0.2971 - val_accuracy: 0.9350 - 363s/epoch - 2s/step\n",
      "Epoch 3/10\n",
      "222/222 - 361s - loss: 0.3587 - accuracy: 0.8519 - val_loss: 0.3162 - val_accuracy: 0.8872 - 361s/epoch - 2s/step\n",
      "Epoch 4/10\n",
      "222/222 - 360s - loss: 0.3245 - accuracy: 0.8609 - val_loss: 0.3094 - val_accuracy: 0.9316 - 360s/epoch - 2s/step\n",
      "Epoch 5/10\n",
      "222/222 - 359s - loss: 0.2870 - accuracy: 0.8849 - val_loss: 0.3297 - val_accuracy: 0.8752 - 359s/epoch - 2s/step\n",
      "Epoch 6/10\n",
      "222/222 - 358s - loss: 0.2645 - accuracy: 0.8895 - val_loss: 0.4096 - val_accuracy: 0.8188 - 358s/epoch - 2s/step\n",
      "Epoch 7/10\n",
      "222/222 - 358s - loss: 0.2344 - accuracy: 0.9002 - val_loss: 0.7058 - val_accuracy: 0.7179 - 358s/epoch - 2s/step\n",
      "Epoch 8/10\n",
      "222/222 - 358s - loss: 0.1926 - accuracy: 0.9149 - val_loss: 0.3060 - val_accuracy: 0.8872 - 358s/epoch - 2s/step\n",
      "Epoch 9/10\n",
      "222/222 - 358s - loss: 0.1947 - accuracy: 0.9183 - val_loss: 0.2840 - val_accuracy: 0.8462 - 358s/epoch - 2s/step\n",
      "Epoch 10/10\n",
      "222/222 - 358s - loss: 0.1674 - accuracy: 0.9279 - val_loss: 0.2082 - val_accuracy: 0.9299 - 358s/epoch - 2s/step\n",
      "Test loss: 1.7259989976882935, Test accuracy 0.7647058963775635\n",
      "[[1.2563328742980957, 0.6470588445663452], [0.9115867614746094, 0.8039215803146362], [2.1073577404022217, 0.6274510025978088], [1.7259989976882935, 0.7647058963775635]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Add more Dense layer neurons in the last hidden layer, increase the epoch size when the best hyperparameters are known.\n",
    "# TODO: Also increase filers in the conv layers, kernel size too maybe and increase the batch size in the data batches to 64 or 128 perhaps to speed things up.\n",
    "# TODO: + maybe not the vgg16 preprocess on the batches?\n",
    "\n",
    "scores_customCNN = []\n",
    "models_customCNN = []\n",
    "\n",
    "for i in combinations:\n",
    "    lr = param_grid['learning_rate'][i[0]]\n",
    "    layers = param_grid['hidden_layers'][i[1]]\n",
    "    print(\"Learning rate:\", lr)\n",
    "    print(\"Layers:\", layers)\n",
    "\n",
    "    model_customCNN = Sequential()\n",
    "    \n",
    "    model_customCNN.add(Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=(5, 5),\n",
    "            activation=\"relu\",\n",
    "            padding=\"same\",\n",
    "            input_shape=(img_height, img_width, 3),\n",
    "        ))\n",
    "    model_customCNN.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "    for i in range(layers):\n",
    "        model_customCNN.add(Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"))\n",
    "        model_customCNN.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "    \n",
    "    model_customCNN.add(GlobalAveragePooling2D())\n",
    "    model_customCNN.add(Dense(64, activation=\"relu\"))\n",
    "\n",
    "    model_customCNN.add(Dense(units=2, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "    model_customCNN.compile(\n",
    "        optimizer=Adam(learning_rate=lr),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    model_customCNN.summary()\n",
    "    \n",
    "    model_customCNN.fit(x=train_batches, validation_data=valid_batches, epochs=10, verbose=2)\n",
    "    score = model_customCNN.evaluate(test_batches, verbose=0)\n",
    "\n",
    "    print(\"Test loss: {}, Test accuracy {}\".format(score[0], score[1]))\n",
    "\n",
    "    scores_customCNN.append(score)\n",
    "    models_customCNN.append(model_customCNN)\n",
    "\n",
    "print(scores_customCNN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained CNN model (VGG16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n",
      "Layers: 1\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AndreiBorg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\preprocessing\\image.py:1863: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443/443 - 158s - loss: 0.3405 - accuracy: 0.8567 - val_loss: 0.3812 - val_accuracy: 0.8718 - 158s/epoch - 357ms/step\n",
      "Epoch 2/5\n",
      "443/443 - 140s - loss: 0.1322 - accuracy: 0.9525 - val_loss: 0.3959 - val_accuracy: 0.8513 - 140s/epoch - 316ms/step\n",
      "Epoch 3/5\n",
      "443/443 - 143s - loss: 0.0850 - accuracy: 0.9757 - val_loss: 0.3541 - val_accuracy: 0.8803 - 143s/epoch - 322ms/step\n",
      "Epoch 4/5\n",
      "443/443 - 141s - loss: 0.0573 - accuracy: 0.9890 - val_loss: 0.3607 - val_accuracy: 0.9009 - 141s/epoch - 319ms/step\n",
      "Epoch 5/5\n",
      "443/443 - 141s - loss: 0.0384 - accuracy: 0.9972 - val_loss: 0.3861 - val_accuracy: 0.8974 - 141s/epoch - 318ms/step\n",
      "Test loss: 0.639529824256897, Test accuracy 0.8627451062202454\n",
      "Learning rate: 0.0001\n",
      "Layers: 2\n",
      "Epoch 1/5\n",
      "443/443 - 145s - loss: 0.2274 - accuracy: 0.9118 - val_loss: 0.2284 - val_accuracy: 0.8991 - 145s/epoch - 327ms/step\n",
      "Epoch 2/5\n",
      "443/443 - 145s - loss: 0.1041 - accuracy: 0.9655 - val_loss: 0.2342 - val_accuracy: 0.9179 - 145s/epoch - 328ms/step\n",
      "Epoch 3/5\n",
      "443/443 - 149s - loss: 0.0547 - accuracy: 0.9881 - val_loss: 0.2574 - val_accuracy: 0.9009 - 149s/epoch - 337ms/step\n",
      "Epoch 4/5\n",
      "443/443 - 139s - loss: 0.0311 - accuracy: 0.9958 - val_loss: 0.2521 - val_accuracy: 0.9026 - 139s/epoch - 315ms/step\n",
      "Epoch 5/5\n",
      "443/443 - 147s - loss: 0.0184 - accuracy: 0.9972 - val_loss: 0.2963 - val_accuracy: 0.9111 - 147s/epoch - 331ms/step\n",
      "Test loss: 1.4803391695022583, Test accuracy 0.6666666865348816\n",
      "Learning rate: 0.001\n",
      "Layers: 1\n",
      "Epoch 1/5\n",
      "443/443 - 143s - loss: 0.1787 - accuracy: 0.9237 - val_loss: 0.4405 - val_accuracy: 0.8889 - 143s/epoch - 323ms/step\n",
      "Epoch 2/5\n",
      "443/443 - 138s - loss: 0.0335 - accuracy: 0.9884 - val_loss: 0.4102 - val_accuracy: 0.9094 - 138s/epoch - 311ms/step\n",
      "Epoch 3/5\n",
      "443/443 - 138s - loss: 0.0118 - accuracy: 0.9994 - val_loss: 0.5682 - val_accuracy: 0.9265 - 138s/epoch - 311ms/step\n",
      "Epoch 4/5\n",
      "443/443 - 136s - loss: 0.0200 - accuracy: 0.9941 - val_loss: 0.8114 - val_accuracy: 0.7829 - 136s/epoch - 307ms/step\n",
      "Epoch 5/5\n",
      "443/443 - 137s - loss: 0.0040 - accuracy: 0.9997 - val_loss: 0.6213 - val_accuracy: 0.9128 - 137s/epoch - 308ms/step\n",
      "Test loss: 1.958091139793396, Test accuracy 0.8039215803146362\n",
      "Learning rate: 0.001\n",
      "Layers: 2\n",
      "Epoch 1/5\n",
      "443/443 - 139s - loss: 0.2188 - accuracy: 0.9172 - val_loss: 0.4640 - val_accuracy: 0.8940 - 139s/epoch - 313ms/step\n",
      "Epoch 2/5\n",
      "443/443 - 137s - loss: 0.0561 - accuracy: 0.9791 - val_loss: 0.4596 - val_accuracy: 0.9248 - 137s/epoch - 308ms/step\n",
      "Epoch 3/5\n",
      "443/443 - 137s - loss: 0.0424 - accuracy: 0.9845 - val_loss: 0.4669 - val_accuracy: 0.9162 - 137s/epoch - 308ms/step\n",
      "Epoch 4/5\n",
      "443/443 - 136s - loss: 0.0469 - accuracy: 0.9859 - val_loss: 0.4051 - val_accuracy: 0.9145 - 136s/epoch - 308ms/step\n",
      "Epoch 5/5\n",
      "443/443 - 136s - loss: 0.0049 - accuracy: 0.9992 - val_loss: 0.5857 - val_accuracy: 0.9145 - 136s/epoch - 307ms/step\n",
      "Test loss: 2.3178205490112305, Test accuracy 0.7450980544090271\n",
      "[[0.639529824256897, 0.8627451062202454], [1.4803391695022583, 0.6666666865348816], [1.958091139793396, 0.8039215803146362], [2.3178205490112305, 0.7450980544090271]]\n"
     ]
    }
   ],
   "source": [
    "scores_vgg16 = []\n",
    "models_vgg16 = []\n",
    "\n",
    "for i in combinations:\n",
    "    lr = param_grid['learning_rate'][i[0]]\n",
    "    layers = param_grid['hidden_layers'][i[1]]\n",
    "    print(\"Learning rate:\", lr)\n",
    "    print(\"Layers:\", layers)\n",
    "\n",
    "    vgg16_model = tf.keras.applications.vgg16.VGG16()\n",
    "    #vgg16_model.summary()\n",
    "\n",
    "    modelVGG = Sequential()\n",
    "    for layer in vgg16_model.layers[:-4]:\n",
    "        modelVGG.add(layer)\n",
    "\n",
    "    for layer in modelVGG.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    modelVGG.add(GlobalAveragePooling2D())\n",
    "    for i in range(layers):\n",
    "        modelVGG.add(Dense(128, activation=\"relu\"))\n",
    "    \n",
    "    modelVGG.add(Dense(units=2, activation=\"softmax\"))\n",
    "\n",
    "    modelVGG.compile(\n",
    "        optimizer=Adam(learning_rate=lr),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    modelVGG.fit(x=train_batches, validation_data=valid_batches, epochs=5, verbose=2)\n",
    "    score = modelVGG.evaluate(test_batches, verbose=0)\n",
    "\n",
    "    print(\"Test loss: {}, Test accuracy {}\".format(score[0], score[1]))\n",
    "\n",
    "    scores_vgg16.append(score)\n",
    "    models_vgg16.append(modelVGG)\n",
    "\n",
    "print(scores_vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = models_vgg16[0].evaluate(test_batches, verbose=0)\n",
    "\n",
    "print(\"Test loss: {}, Test accuracy: {}\".format(score[0], score[1]))\n",
    "\n",
    "predictions = models_vgg16[0].predict(x=test_batches, verbose=0)\n",
    "\n",
    "np.round(predictions)\n",
    "\n",
    "cm = confusion_matrix(\n",
    "    y_true=test_batches.classes, y_pred=np.argmax(predictions, axis=-1)\n",
    ")\n",
    "\n",
    "\n",
    "# extract true positives, false positives, true negatives, and false negatives\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# calculate sensitivity and specificity\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "print(\"Specificity:\", specificity)\n",
    "\n",
    "cm_plot_labels = [\"normal\", \"stroke\"]\n",
    "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title=\"Confusion Matrix\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained RNN model (ResNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n",
      "Layers: 1\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AndreiBorg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "c:\\Users\\AndreiBorg\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\preprocessing\\image.py:1863: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443/443 - 196s - loss: 0.2875 - accuracy: 0.8835 - val_loss: 0.3340 - val_accuracy: 0.9316 - 196s/epoch - 443ms/step\n",
      "Epoch 2/5\n",
      "443/443 - 198s - loss: 0.1667 - accuracy: 0.9386 - val_loss: 0.2867 - val_accuracy: 0.9179 - 198s/epoch - 446ms/step\n",
      "Epoch 3/5\n",
      "443/443 - 184s - loss: 0.1186 - accuracy: 0.9579 - val_loss: 0.2732 - val_accuracy: 0.9282 - 184s/epoch - 415ms/step\n",
      "Epoch 4/5\n",
      "443/443 - 184s - loss: 0.0855 - accuracy: 0.9765 - val_loss: 0.2516 - val_accuracy: 0.9350 - 184s/epoch - 414ms/step\n",
      "Epoch 5/5\n",
      "443/443 - 184s - loss: 0.0633 - accuracy: 0.9864 - val_loss: 0.2983 - val_accuracy: 0.9368 - 184s/epoch - 415ms/step\n",
      "Test loss: 0.549350380897522, Test accuracy 0.8235294222831726\n",
      "Learning rate: 0.0001\n",
      "Layers: 2\n",
      "Epoch 1/5\n",
      "443/443 - 202s - loss: 0.3233 - accuracy: 0.8674 - val_loss: 0.2809 - val_accuracy: 0.9265 - 202s/epoch - 456ms/step\n",
      "Epoch 2/5\n",
      "443/443 - 192s - loss: 0.1694 - accuracy: 0.9344 - val_loss: 0.2614 - val_accuracy: 0.9419 - 192s/epoch - 433ms/step\n",
      "Epoch 3/5\n",
      "443/443 - 185s - loss: 0.1011 - accuracy: 0.9675 - val_loss: 0.2527 - val_accuracy: 0.9402 - 185s/epoch - 417ms/step\n",
      "Epoch 4/5\n",
      "443/443 - 181s - loss: 0.0610 - accuracy: 0.9811 - val_loss: 0.2290 - val_accuracy: 0.9385 - 181s/epoch - 409ms/step\n",
      "Epoch 5/5\n",
      "443/443 - 181s - loss: 0.0382 - accuracy: 0.9926 - val_loss: 0.2440 - val_accuracy: 0.9385 - 181s/epoch - 409ms/step\n",
      "Test loss: 0.8068253993988037, Test accuracy 0.8235294222831726\n",
      "Learning rate: 0.001\n",
      "Layers: 1\n",
      "Epoch 1/5\n",
      "443/443 - 201s - loss: 0.2236 - accuracy: 0.9107 - val_loss: 0.3154 - val_accuracy: 0.9162 - 201s/epoch - 453ms/step\n",
      "Epoch 2/5\n",
      "443/443 - 194s - loss: 0.1315 - accuracy: 0.9491 - val_loss: 0.2737 - val_accuracy: 0.9162 - 194s/epoch - 437ms/step\n",
      "Epoch 3/5\n",
      "443/443 - 188s - loss: 0.0463 - accuracy: 0.9845 - val_loss: 0.3093 - val_accuracy: 0.9333 - 188s/epoch - 424ms/step\n",
      "Epoch 4/5\n",
      "443/443 - 188s - loss: 0.0444 - accuracy: 0.9828 - val_loss: 0.2075 - val_accuracy: 0.9350 - 188s/epoch - 425ms/step\n",
      "Epoch 5/5\n",
      "443/443 - 186s - loss: 0.0212 - accuracy: 0.9935 - val_loss: 0.3368 - val_accuracy: 0.9333 - 186s/epoch - 421ms/step\n",
      "Test loss: 0.8709962368011475, Test accuracy 0.843137264251709\n",
      "Learning rate: 0.001\n",
      "Layers: 2\n",
      "Epoch 1/5\n",
      "443/443 - 191s - loss: 0.2754 - accuracy: 0.8883 - val_loss: 0.3664 - val_accuracy: 0.9094 - 191s/epoch - 432ms/step\n",
      "Epoch 2/5\n",
      "443/443 - 182s - loss: 0.1004 - accuracy: 0.9582 - val_loss: 0.3889 - val_accuracy: 0.8940 - 182s/epoch - 410ms/step\n",
      "Epoch 3/5\n",
      "443/443 - 182s - loss: 0.0541 - accuracy: 0.9808 - val_loss: 0.3026 - val_accuracy: 0.9333 - 182s/epoch - 410ms/step\n",
      "Epoch 4/5\n",
      "443/443 - 182s - loss: 0.0476 - accuracy: 0.9842 - val_loss: 0.4728 - val_accuracy: 0.9368 - 182s/epoch - 411ms/step\n",
      "Epoch 5/5\n",
      "443/443 - 181s - loss: 0.0464 - accuracy: 0.9853 - val_loss: 0.3840 - val_accuracy: 0.9333 - 181s/epoch - 410ms/step\n",
      "Test loss: 1.3583650588989258, Test accuracy 0.8235294222831726\n",
      "[[0.549350380897522, 0.8235294222831726], [0.8068253993988037, 0.8235294222831726], [0.8709962368011475, 0.843137264251709], [1.3583650588989258, 0.8235294222831726]]\n"
     ]
    }
   ],
   "source": [
    "input_shape = (465, 930, 3)\n",
    "scores_resnet = []\n",
    "models_resnet = []\n",
    "\n",
    "for i in combinations:\n",
    "    lr = param_grid['learning_rate'][i[0]]\n",
    "    layers = param_grid['hidden_layers'][i[1]]\n",
    "    print(\"Learning rate:\", lr)\n",
    "    print(\"Layers:\", layers)\n",
    "\n",
    "    # Create the base ResNet50 model\n",
    "    base_model = ResNet152(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "\n",
    "    # Add a global average pooling layer and a dense layer with softmax activation for classification\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    for i in range(layers):\n",
    "        x = Dense(128, activation=\"relu\")(x)\n",
    "    predictions = Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    # Define the model to be trained with the ResNet50 base and the classification layers added\n",
    "    model_resnet = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Freeze the base ResNet50 layers so they are not updated during training\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Compile the model with a learning rate of 0.001 and a categorical cross-entropy loss function\n",
    "    model_resnet.compile(\n",
    "        optimizer=Adam(lr=lr), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    model_resnet.fit(x=train_batches, validation_data=valid_batches, epochs=5, verbose=2)\n",
    "    score = model_resnet.evaluate(test_batches, verbose=0)\n",
    "\n",
    "    print(\"Test loss: {}, Test accuracy {}\".format(score[0], score[1]))\n",
    "    \n",
    "    scores_resnet.append(score)\n",
    "    models_resnet.append(model_resnet)\n",
    "\n",
    "print(scores_resnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.8709962368011475, Test accuracy: 0.843137264251709\n",
      "Sensitivity: 0.8461538461538461\n",
      "Specificity: 0.84\n",
      "Confusion matrix, without normalization\n",
      "[[21  4]\n",
      " [ 4 22]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score = models_resnet[2].evaluate(test_batches, verbose=0)\n",
    "\n",
    "print(\"Test loss: {}, Test accuracy: {}\".format(score[0], score[1]))\n",
    "\n",
    "predictions = models_resnet[2].predict(x=test_batches, verbose=0)\n",
    "\n",
    "np.round(predictions)\n",
    "\n",
    "cm = confusion_matrix(\n",
    "    y_true=test_batches.classes, y_pred=np.argmax(predictions, axis=-1)\n",
    ")\n",
    "\n",
    "# extract true positives, false positives, true negatives, and false negatives\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# calculate sensitivity and specificity\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "print(\"Specificity:\", specificity)\n",
    "\n",
    "cm_plot_labels = [\"normal\", \"stroke\"]\n",
    "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title=\"Confusion Matrix\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of classes has to be greater than one; got 1 class",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rbf \u001b[39m=\u001b[39m svm\u001b[39m.\u001b[39;49mSVC(kernel\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrbf\u001b[39;49m\u001b[39m'\u001b[39;49m, gamma\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m, C\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(train_images, train_labels)\n\u001b[0;32m      2\u001b[0m rbf_pred \u001b[39m=\u001b[39m rbf\u001b[39m.\u001b[39mpredict(test_images)\n\u001b[0;32m      3\u001b[0m svm_score \u001b[39m=\u001b[39m classification_report(test_labels, rbf_pred)\n",
      "File \u001b[1;32mc:\\Users\\AndreiBorg\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\svm\\_base.py:201\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    192\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[0;32m    193\u001b[0m         X,\n\u001b[0;32m    194\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    198\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_targets(y)\n\u001b[0;32m    203\u001b[0m sample_weight \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\n\u001b[0;32m    204\u001b[0m     [] \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m sample_weight, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64\n\u001b[0;32m    205\u001b[0m )\n\u001b[0;32m    206\u001b[0m solver_type \u001b[39m=\u001b[39m LIBSVM_IMPL\u001b[39m.\u001b[39mindex(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impl)\n",
      "File \u001b[1;32mc:\\Users\\AndreiBorg\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\svm\\_base.py:749\u001b[0m, in \u001b[0;36mBaseSVC._validate_targets\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_weight_ \u001b[39m=\u001b[39m compute_class_weight(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_weight, classes\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m, y\u001b[39m=\u001b[39my_)\n\u001b[0;32m    748\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mcls\u001b[39m) \u001b[39m<\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m--> 749\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    750\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe number of classes has to be greater than one; got \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m class\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    751\u001b[0m         \u001b[39m%\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mcls\u001b[39m)\n\u001b[0;32m    752\u001b[0m     )\n\u001b[0;32m    754\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\n\u001b[0;32m    756\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray(y, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64, order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: The number of classes has to be greater than one; got 1 class"
     ]
    }
   ],
   "source": [
    "rbf = svm.SVC(kernel='rbf', gamma=0.5, C=0.1).fit(train_images, train_labels)\n",
    "rbf_pred = rbf.predict(test_images)\n",
    "svm_score = classification_report(test_labels, rbf_pred)\n",
    "rbf_accuracy = accuracy_score(test_labels, rbf_pred)\n",
    "print('Accuracy (RBF Kernel): ', \"%.2f\" % (rbf_accuracy*100))\n",
    "print(svm_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(rbf_pred)\n",
    "\n",
    "cm = confusion_matrix(\n",
    "    y_true=test_labels, y_pred=np.argmax(rbf_pred, axis=-1)\n",
    ")\n",
    "\n",
    "# extract true positives, false positives, true negatives, and false negatives\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# calculate sensitivity and specificity\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "print(\"Specificity:\", specificity)\n",
    "\n",
    "cm_plot_labels = [\"normal\", \"stroke\"]\n",
    "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title=\"Confusion Matrix\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ... and Grid Search version of the SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf = svm.SVC(kernel='rbf')\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid_search = GridSearchCV(rbf, param_grid, cv=5)\n",
    "grid_search.fit(train_images, train_labels)\n",
    "\n",
    "# Evaluate the best model on the validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "valid_pred = best_model.predict(valid_images)\n",
    "valid_accuracy = accuracy_score(valid_labels, valid_pred)\n",
    "valid_score = classification_report(valid_labels, valid_pred)\n",
    "\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "print('Validation accuracy (RBF kernel): ', \"%.2f\" % (valid_accuracy*100))\n",
    "print(valid_score)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_pred = best_model.predict(test_images)\n",
    "test_accuracy = accuracy_score(test_labels, test_pred)\n",
    "test_score = classification_report(test_labels, test_pred)\n",
    "\n",
    "print('Test accuracy (RBF kernel): ', \"%.2f\" % (test_accuracy*100))\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(test_pred)\n",
    "\n",
    "cm = confusion_matrix(\n",
    "    y_true=test_labels, y_pred=np.argmax(test_pred, axis=-1)\n",
    ")\n",
    "\n",
    "# extract true positives, false positives, true negatives, and false negatives\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# calculate sensitivity and specificity\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "print(\"Specificity:\", specificity)\n",
    "\n",
    "cm_plot_labels = [\"normal\", \"stroke\"]\n",
    "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title=\"Confusion Matrix\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
